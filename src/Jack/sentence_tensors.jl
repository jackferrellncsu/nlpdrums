using JLD
using Embeddings
using DataFrames
using Lathe.preprocess: TrainTestSplit
using Flux
using LinearAlgebra
using Plots
using Statistics
using Random
using StatsBase
using InvertedIndices
using BSON

Random.seed!(26)
#----------Prepare Embeddings---------------------------------------#
obj = load("PridePrej.jld")
    data = obj["data"]
    sentences = obj["sentances"]
    corpus = obj["corpus"]

embtable = load("pridePrejEmbs.jld", "embtable")
#get vector from word
get_vector_word = Dict(word=>embtable.embeddings[:,ii] for (ii,word) in enumerate(embtable.vocab))
#get word from vector
get_word_vector = Dict(embtable.embeddings[:, ii] => word for (ii, word) in enumerate(embtable.vocab))
get_word_index = Dict(word=>ii for (ii, word) in enumerate(embtable.vocab))
vec_length = length(embtable.embeddings[:, get_word_index["the"]])

x_mat, y_mat = EmbeddingsTensor(data)

#split into test, proper_train, calibrate
train_x, test_x, train_y, test_y = SampleMats(x_mat, y_mat)
proper_train_x, calibrate_x, proper_train_y, calibrate_y = SampleMats(train_x, train_y, .92)

trainDL = Flux.Data.DataLoader((proper_train_x, proper_train_y),
                            batchsize = 100,
                            shuffle = true)

calibrateDL = Flux.Data.DataLoader((calibrate_x, calibrate_y))
testDL = Flux.Data.DataLoader((test_x, test_y))

nn = Chain(Flux.flatten,
           Dense(1500, 800, mish),
           Dense(800, 500, mish),
           Dense(500, 300, x->x))

opt = RADAM(1e-4)
ps = Flux.params(nn)

epochs = 100
trace = TrainNN!(epochs)

plot(1:epochs, trace)

err = 0
for (x, y) in calibrateDL
    err += norm(y - nn(x))^2
end
mse_acc = err / length(calibrateDL.data[2][1, :])

using BSON: @save
@save "basic.bson" nn

using BSON: @load

BSON.@load "basic.bson" nn


test = conf_pred(nn, 0.3)

accuracy = CheckValidity(test)
accuracy

"""
    CheckValidity(intervals)

Checks how many values actually lie in the confidence regions generated by ICP
"""
function CheckValidity(intervals)
    acc = 0
    for (i, region) in enumerate(intervals)
        if mean(test_y[:,i]) != 0 && get_word_vector[test_y[:, i]] ∈ region
            acc += 1
        end
    end
    return acc / length(intervals)

end

"""
    ConfPred(nn, ϵ = 0.05)

Given a neural net, performes inductive conformal prediction and returns prediction
regions for test set.
"""
function ConfPred(nn, ϵ = 0.05)
    α = Vector{Float64}()
    for (x, y) in calibrateDL
        α_i = norm(y - nn(x))
        push!(α, α_i)
    end
    println("Noncomformity scores calculated")
    all_regs = Vector{Vector{String}}()
    for (x, y) in testDL
        a_k = norm(y - nn(x))
        push!(α, a_k)
        q = quantile(α, 1-ϵ)
        region = Vector{String}()
        pred = nn(x)
        for i in get_vector_word
            dist = norm(pred - i[2])
            if dist <= q
                push!(region, i[1])
            end
        end
        pop!(α)
        push!(all_regs, region)
        print(length(region))
    end
    return all_regs
end


α = Vector{Float64}()
for (x, y) in calibrateDL
    α_i = norm(y - nn(x))
    push!(α, α_i)
end

sort!(α, rev = true)

α_k = 0
region = []
for (i, (x, y)) in enumerate(testDL)
    if i == 1
        global α_k = norm(y - nn(x))
        push!(α, α_k)
        #c = count(x->(x>=α_k), α)
        print(typeof(x))
        q = quantile(α, 0.50)
        pred = nn(x)
        for i in get_vector_word
            dist = norm(pred - i[2])
            if dist <= q
                push!(region, i[1])
            end
        end

    else
        break
    end
end

test = zeros(300, 5, 1)
for i in 1:5
    test[:, i] = getEmbedding("forbade")
end
q = quantile(α, 0.5)
pred = nn(test)
region = []
for i in get_vector_word
    dist = norm(pred - i[2])

    if dist <= q
        push!(region, i[1])
    end
end










3560 / (length(α) + 1)

interval = filter(x->(x<=α_k), α)





#-----------------NN Helpers---------------------------------------#
function loss(x, y)
    return norm(nn(x) - y)
end

"""
    TrainNN!(epochs)

Trains a neural net using the specified number of epochs.

Returns a trace plot.
"""
function TrainNN!(epochs)
    traceY = []
    for i in 1:epochs
        Flux.train!(loss, ps, trainDL, opt)
        println(i)
        totalLoss = 0
        for (x,y) in trainDL
         totalLoss += loss(x,y)
         #println("Total Loss: ", totalLoss)
        end
        push!(traceY, totalLoss)
    end
    return traceY
end

#---------Create Tensor--------------------------------------------------------#
"""
    EmbeddingsTensor(data, context_size = 5)

Creates a 3-dimensional cluster
"""
function EmbeddingsTensor(data, context_size = 5)
    tensor = zeros(300, context_size, size(data)[1])

    result = zeros(300, size(data)[1])

    for (i, r) in enumerate(eachrow(data))
        sentence_mat = zeros(300, context_size)
        if length(r[1]) >= context_size
            for (j, w) in enumerate(r[1][end-4:end])
                sentence_mat[:, j] = getEmbedding(w)
            end
        else
            sent_length = length(r[1])
            for j in 1:context_size
                if j <= sent_length
                    sentence_mat[:, j] = getEmbedding(r[1][j])
                else
                    sentence_mat[:, j] = mean.(eachrow(sentence_mat[:, 1:sent_length]))
                end
            end
        end
        tensor[:, :, i] = sentence_mat
        result[:, i] = getEmbedding(r[2])
    end
    return tensor, result
end

#returns trainx, testx, trainy, testy
function SampleMats(x_mat, y_mat, prop = 0.9)
    inds = [1:size(x_mat)[3];]
    length(inds)
    trains = sample(inds, Int(floor(length(inds) * prop)), replace = false)
    inds = Set(inds)
    trains = Set(trains)
    tests = setdiff(inds, trains)

    train_x = x_mat[:, :, collect(trains)]
    train_y = y_mat[:, collect(trains)]

    test_x = x_mat[:, :, collect(tests)]
    test_y = y_mat[:, collect(tests)]


    return train_x, test_x, train_y, test_y
end

function getEmbedding(word)
    if word ∈ keys(get_word_index)
        ind = get_word_index[word]
        emb = embtable.embeddings[:, ind]
    else
        vec_length = length(embtable.embeddings[:, get_word_index["the"]])
        emb = zeros(vec_length)
    end
    return emb
end
